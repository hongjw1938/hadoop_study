##  6장. 빅데이터 탐색
>###    6.1 빅데이터 탐색 개요
        ** 처리 및 탐색
            * 개념
                - 적재된 데이터 가공 및 이해
                - 데이터의 패턴, 관계, 트렌드 탐색(탐색적 분석, EDA : Exploratory Data Analysis)
                - 비정형 데이터 정교한 후처리 작업 -> 정형화
                - 데이터 직관성 확보, 비즈니스 가치 창출
            * 빅데이터 DW(Data Warehouse)
                - 전통적 RDBMS 기반의 DW구성과 유사
                - External영역은 전처리와 후처리가 만나 데이터 서로 공유하는 영역, 원천 데이터 형식 유지
                - External의 데이터셋은 처리/가공을 거쳐 Managed영역으로 이동, 주제 영역별 처리/탐색 과정 거쳐 빅데이터 분석용 마트 최종 생성
                - 마트 모델을 통합, 요약, 집계 등을 리포팅하는 현황 분석 모형으로 만들거나 패턴, 트렌드 분석으로 고급 분석 모형 생성 가능
>###    6.2 빅데이터 탐색에 활용되는 기술
        ** 하이브
            * 소개
                - SQL과 매우 유사한 방식으로 하둡 데이터에 접근성을 높인 프로그램
            * 주요 구성 요소
                - CLI : 하이브 쿼리 입력하고 실행할 수 있는 인터페이스
                - JDBC/ODBC Driver : 하이브의 쿼리를 다양한 데이터베이스와 연결하기 위한 드라이버 제공
                - Query Engine : 사용자가 입력한 하이브 쿼리를 분석해 실행 계획 수립. 하이브 QL을 맵리듀스 코드로 변환 및 실행
                - MetaStore : 하이브에서 사용하는 테이블의 스키마 정보를 저장 및 관리. 기본적으로 더비 사용
            * 아키덱쳐
                - CLI, 웹 콘솔 등을 통해 하이브 QL을 작성시 쿼리 엔진에 있는 SQL 파서가 하이브 QL을 맵리듀스 프로그램으로 변환
                - 맵리듀스 프로그램은 하둡 클러스터에 전송되어 여러 데이터노드에서 분산 실행
                - MetaStore에는 하이브 DW에서 정의한 데이터베이스, 테이블, 파티션 정보 저장
                - Thrift API를 호출해 하이브 액션 외부 수행 가능
            * 활용방안
                - 스마트카 데이터셋을 다양한 각도로 탐색 및 가공하는데 활용
                - 하이브 QL로 스마트카 데이터에 대한 조회, 결합, 분리, 변환, 정제 작업 수행 --> 스마트카 DW 구성
                - 2, 3차 탐색 및 고급 분석을 거쳐 스마트카 분석마트 생성
        ** 스파크
            * 소개
                - 하이브는 맵리듀스 코어를 그대로 사용하여 성능면에서 부족
                - 반복적인 대화형 연산 작업에 적합하도록 스파크 개발
            * 주요 구성 요소
                - Spark RDD : 스파크 프로그래밍의 기초 데이터셋 모델
                - Spark Driver / Executors : Driver는 RDD 프로그램을 분산 노드에서 실행하기 위한 Task 구성, 할당, 계획 수립. Executors는 Task를 실행 관리
                - Spark Cluster Manager : 스파크 실행 환경을 구성하는 클러스터 관리자
                - Spark SQL : SQL방식으로 스파크 RDD 프로그래밍 지원
                - Spark Streaming : 스트리밍 데이터를 마이크로타임의 배치로 나누어 실시간 처리
                - Spark MLib : 스파크에서 머신러닝 프로그래밍 지원
                - Spark GraphX : 다양한 유형의 네트워크 구조 분석 지원
            * 아키텍쳐
                - 데이터 가공 처리를 인메모리에서 수행 -> 대용량 데이터 작업에서 빠른 수행 보장
                - 스파크의 분산 노드로 아파치 메소스, 하둡 얀을 이용
                - 데이터소스 영역은 HDFS, HBase, 카산드라, 일래스틱 서치 연결해 이용가능
            * 활용방안
                - 스파크 셸에서 스파크-SQL API를 이용해 추가로 적재될 '스마트카 마스터' 데이터를 조회 및 정제함
        ** 우지
            * 소개
                - 반복적이면서 복잡한 후처리 잡을 처리하기 위해 방향성 있는 비순환 그래프로 정의해 잡에 시작, 처리, 분기, 종료점 등의 액션으로 구성하는 워크플로
            * 주요 구성 요소
                - Oozie Workflow : 주요 액션에 대한 작업 규칙과 플로우 정의
                - Oozie Client : 워크플로를 Server에 전송하고 관리하기 위한 환경
                - Oozie Server : 워크플로 정보가 잡으로 등록되어 잡의 실행, 중지, 모니터링 등을 관리
                - Control 노드 : 워크플로의 흐름을 제어하기 위한 Start, End, Decision 노드 등의 기능 제공
                - Action 노드 : 잡의 실제 수행 태스크를 정의하는 노드
                - Coordinator : 워크플로 잡을 실행하기 위한 스케줄 정책 관리
            * 아키텍쳐
                - 우지의 클라이언트에서 작성한 워크플로는 우지 서버에 전송, 관련 워크플로 메타 정보는 RDBMS에서 별도 관리
                - Coordinator는 우지에 등록된 워크플로를 스케줄링하며, 이때 워크플로 엔진지 Action노드와 Control노드의 정보 해석하여 관련 태스크를 하둡의 클러스터에서 실행시킴
                - 실행 중인 태스크의 라이프 사이클을 우지 서버가 시작부터 종료까지 추적하여 모니터링 정보 제공.
            * 활용방안
                - 하이브 QL을 사용할 때 스케줄링이 필요함. 이 때, 우지의 워크플로 사용
        ** 휴
            * 소개
                - 다양한 하둡의 에코시스템의 기능을 웹 UI로 통합 제공.
            * 주요 구성 요소
                - Job Designer : 우지의 워크플로 및 Coordinator를 웹 UI에서 디자인
                - Job Browser : 등록한 잡의 리스트 및 진행 상황과 결과 등 조회
                - Hive Editor : 하이브 QL을 웹 UI에서 작성, 실행, 관리
                - Pig Editor : 피그 스크립트를 웹 UI에서 작성, 실행 관리
                - HDFS Browser : 하둡의 파일시스템을 웹 UI에서 탐색 및 관리
                - HBase Browser : HBase의 HTable을 웹 UI에서 탐색 및 관리
            * 아키텍쳐
                - 하둡 에코시스템 통합 위해 자체 플러그인 설치하거나 API를 연동하여 에코시스템들의 주요 기능들을 웹 UI로 제공
            * 활용방안
                - 웹 에디터를 이용해 "스마트카 상태 데이터". "스마트카 운전자의 운행 데이터" 등을 직관적으로 탐색
                - "스마트카 마스터 데이터" 및 "스마트카 차량 물품 구매 이력 데이터"를 임포트하는 작업 수행
                - 휴의 Job Designer를 이용해 우지의 워크플로를 5개의 주제 영역별로 작성하고 실행
>###    6.3 